{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "#torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "#torchvision\n",
    "import torchvision.models as models\n",
    "\n",
    "#image\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "#jupyter\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "#os\n",
    "import os\n",
    "import os.path as path\n",
    "import glob\n",
    "\n",
    "#math\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ConvLSTM\n",
    "\n",
    "#### LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "t = Variable(torch.rand(1,256,6,6))\n",
    "ht = Variable( torch.zeros(1,128,6,6))\n",
    "ct = Variable( torch.zeros(1,128,6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=1, stride=1, bias=False):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        \n",
    "        self.k = kernel_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.w_i = nn.Parameter(torch.Tensor(4*out_channels, in_channels, kernel_size, kernel_size))\n",
    "        self.w_h = nn.Parameter(torch.Tensor(4*out_channels, out_channels, kernel_size, kernel_size))\n",
    "        self.w_c = nn.Parameter(torch.Tensor(3*out_channels, out_channels, kernel_size, kernel_size))\n",
    "\n",
    "        self.bias = bias\n",
    "        if bias:\n",
    "          self.bias_i = Parameter(torch.Tensor(4 * out_channels))\n",
    "          self.bias_h = Parameter(torch.Tensor(4 * out_channels))\n",
    "          self.bias_c = Parameter(torch.Tensor(3 * out_channels))\n",
    "        else:\n",
    "          self.register_parameter('bias_i', None)\n",
    "          self.register_parameter('bias_h', None)\n",
    "          self.register_parameter('bias_c', None)\n",
    "        \n",
    "        self.register_buffer('wc_blank', torch.zeros(out_channels))\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        n = 4 * self.in_channels * self.k * self.k\n",
    "        stdv = 1. / math.sqrt(n)\n",
    "        \n",
    "        self.w_i.data.uniform_(-stdv, stdv)\n",
    "        self.w_h.data.uniform_(-stdv, stdv)\n",
    "        self.w_c.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "        if self.bias:\n",
    "            self.bias_i.data.uniform_(-stdv, stdv)\n",
    "            self.bias_h.data.uniform_(-stdv, stdv)\n",
    "            self.bias_c.data.uniform_(-stdv, stdv)\n",
    "\n",
    "        \n",
    "    def forward(self, x, hx):\n",
    "        h, c = hx\n",
    "        wx = F.conv2d(x, self.w_i, self.bias_i, padding=self.padding, stride=self.stride)\n",
    "        wh = F.conv2d(h, self.w_h, self.bias_h, padding=self.padding, stride=self.stride)\n",
    "        wc = F.conv2d(c, self.w_c, self.bias_c, padding=self.padding, stride=self.stride)\n",
    "        \n",
    "        \n",
    "        #wc = torch.cat((wc[:, :2 * self.out_channels], Variable(self.wc_blank).expand(wc.size(0), wc.size(1) // 3, wc.size(2), wc.size(3)), wc[:, 2 * self.out_channels:]), 1)\n",
    "        \n",
    "        i = F.sigmoid(wx[:, :self.out_channels] + wh[:, :self.out_channels] + wc[:, :self.out_channels])\n",
    "        f = F.sigmoid(wx[:, self.out_channels:2*self.out_channels] + wh[:, self.out_channels:2*self.out_channels] \n",
    "                + wc[:, self.out_channels:2*self.out_channels])\n",
    "        g = F.tanh(wx[:, 2*self.out_channels:3*self.out_channels] + wh[:, 2*self.out_channels:3*self.out_channels])\n",
    "        \"\"\"\n",
    "        \n",
    "        wxhc = wx + wh + torch.cat((wc[:, :2 * self.out_channels], Variable(self.wc_blank).expand(wc.size(0), wc.size(1) // 3, wc.size(2), wc.size(3)), wc[:, 2 * self.out_channels:]), 1)\n",
    "    \n",
    "        i = F.sigmoid(wxhc[:, :self.out_channels])\n",
    "        f = F.sigmoid(wxhc[:, self.out_channels:2 * self.out_channels])\n",
    "        g = F.tanh(wxhc[:, 2 * self.out_channels:3 * self.out_channels])\n",
    "        o = F.sigmoid(wxhc[:, 3 * self.out_channels:])\n",
    "        \"\"\"\n",
    "\n",
    "        c_t = f * c + i * g\n",
    "        o_t = F.sigmoid(wx[:, 3*self.out_channels:] + wh[:, 3*self.out_channels:] \n",
    "                        + wc[:, 2*self.out_channels: ]*c_t)\n",
    "        h_t = o_t * F.tanh(c_t)\n",
    "        \n",
    "        return h_t, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Test convLSTM Cell\n",
    "\"\"\"\n",
    "def testconvLSTMCell():\n",
    "    c = ConvLSTMCell(256,128,3)\n",
    "    o = c(t, (ht,ct))\n",
    "    print(o[0].size() == torch.Size([1,128,6,6]))\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import init\n",
    "\n",
    "class ConvGRUCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Generate a convolutional GRU cell\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, kernel_size):\n",
    "        super(ConvGRUCell, self).__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.reset_gate = nn.Conv2d(input_size + hidden_size, hidden_size, kernel_size, padding=padding)\n",
    "        self.update_gate = nn.Conv2d(input_size + hidden_size, hidden_size, kernel_size, padding=padding)\n",
    "        self.out_gate = nn.Conv2d(input_size + hidden_size, hidden_size, kernel_size, padding=padding)\n",
    "\n",
    "        init.orthogonal(self.reset_gate.weight)\n",
    "        init.orthogonal(self.update_gate.weight)\n",
    "        init.orthogonal(self.out_gate.weight)\n",
    "        init.constant(self.reset_gate.bias, 0.)\n",
    "        init.constant(self.update_gate.bias, 0.)\n",
    "        init.constant(self.out_gate.bias, 0.)\n",
    "\n",
    "\n",
    "    def forward(self, input_, prev_state):\n",
    "\n",
    "        # get batch and spatial sizes\n",
    "        batch_size = input_.data.size()[0]\n",
    "        spatial_size = input_.data.size()[2:]\n",
    "\n",
    "        # generate empty prev_state, if None is provided\n",
    "        if prev_state is None:\n",
    "            state_size = [batch_size, self.hidden_size] + list(spatial_size)\n",
    "            if torch.cuda.is_available():\n",
    "                prev_state = Variable(torch.zeros(state_size)).cuda()\n",
    "            else:\n",
    "                prev_state = Variable(torch.zeros(state_size))\n",
    "\n",
    "        # data size is [batch, channel, height, width]\n",
    "        stacked_inputs = torch.cat([input_, prev_state], dim=1)\n",
    "        update = F.sigmoid(self.update_gate(stacked_inputs))\n",
    "        reset = F.sigmoid(self.reset_gate(stacked_inputs))\n",
    "        out_inputs = F.tanh(self.out_gate(torch.cat([input_, prev_state * reset], dim=1)))\n",
    "        new_state = prev_state * (1 - update) + out_inputs * update\n",
    "\n",
    "        return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Test GruCell\n",
    "\"\"\"\n",
    "def testGruCell():\n",
    "    c = ConvGRUCell(256,128,3)\n",
    "    o = c(t, Variable( torch.zeros(1,128,6,6)))\n",
    "    print(o.size() == torch.Size([1,128,6,6]))\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### ConvRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class convRNN_1_layer(nn.Module):\n",
    "    \"\"\"\n",
    "        Define a RNN with 1 recurrent layer\n",
    "        args : r_type : lstm | gru\n",
    "    \"\"\"\n",
    "    def __init__(self, r_type=\"lstm\", copyParameters=False):\n",
    "        super(convRNN_1_layer, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.isCuda = False\n",
    "        self.r_type = r_type\n",
    "        if r_type == \"lstm\":\n",
    "            self.convRNN = ConvLSTMCell(256,128,kernel_size=3, padding=1, stride=1)\n",
    "        elif r_type == \"gru\":\n",
    "            self.convRNN = ConvGRUCell(256,128,3)\n",
    "        else:\n",
    "            print(\"Error : r_type\")\n",
    "            return -1\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(128,6,kernel_size=1, padding=0, stride=1),\n",
    "            nn.AvgPool2d(kernel_size=6, stride=1, padding=0)\n",
    "        )\n",
    "        \n",
    "        if copyParameters:\n",
    "            self.copyParameters(models.alexnet(pretrained=True))\n",
    "        \n",
    "        \n",
    "    def copyParameters(self, alexnetModel):\n",
    "        for i, a in enumerate(alexnetModel.features):\n",
    "            if type(a) is torch.nn.modules.conv.Conv2d:\n",
    "                self.features[i].weight = a.weight\n",
    "                self.features[i].bias   = a.bias\n",
    "    \n",
    "    def cuda(self, device=None):\n",
    "        self.isCuda = True\n",
    "        return self._apply(lambda t: t.cuda(device))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batchSize = len(x[0])\n",
    "        if self.r_type==\"lstm\":\n",
    "            outputs = []\n",
    "            \n",
    "            if self.isCuda:\n",
    "                ht = Variable( torch.zeros(batchSize,128,6,6)).cuda()\n",
    "                ct = Variable( torch.zeros(batchSize,128,6,6)).cuda()\n",
    "            else:\n",
    "                ht = Variable( torch.zeros(batchSize,128,6,6))\n",
    "                ct = Variable( torch.zeros(batchSize,128,6,6))\n",
    "                \n",
    "            for i in x:\n",
    "                xt = self.features(i)\n",
    "                o, (ht,ct) = self.convRNN(xt, (ht, ct))\n",
    "                outputs.append(o)\n",
    "        \n",
    "        elif self.r_type==\"gru\":\n",
    "            outputs = []\n",
    "            if self.isCuda:\n",
    "                ht = Variable( torch.zeros(batchSize,128,6,6)).cuda()\n",
    "            else:\n",
    "                ht = Variable( torch.zeros(batchSize,128,6,6))\n",
    "            \n",
    "            for e,i in enumerate(x):\n",
    "                xt = self.features(i)\n",
    "                ht = self.convRNN(xt, ht)\n",
    "                outputs.append(ht)\n",
    "        \n",
    "        return self.classifier(outputs[-1]).view(batchSize,-1)\n",
    "        #x = self.classifier(x).squeeze().unsqueeze(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class convRNN_2_layer(nn.Module):\n",
    "    \"\"\"\n",
    "        Define a RNN with 1 recurrent layer\n",
    "        args : r_type : lstm | gru\n",
    "    \"\"\"\n",
    "    def __init__(self, r_type=\"lstm\", copyParameters=False):\n",
    "        super(convRNN_2_layer, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.isCuda = False\n",
    "        self.r_type = r_type\n",
    "        if r_type == \"lstm\":\n",
    "            self.convRNN1 = ConvLSTMCell(256,256,kernel_size=3, padding=1, stride=1)\n",
    "            self.convRNN2 = ConvLSTMCell(256,128,kernel_size=3, padding=1, stride=1)\n",
    "        elif r_type == \"gru\":\n",
    "            self.convRNN1 = ConvGRUCell(256,256,3)\n",
    "            self.convRNN2 = ConvGRUCell(256,128,3)\n",
    "        else:\n",
    "            print(\"Error : r_type\")\n",
    "            return -1\n",
    "        \n",
    "        \"\"\"\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(128,6,kernel_size=1, padding=0, stride=1),\n",
    "            nn.AvgPool2d(kernel_size=6, stride=1, padding=0)\n",
    "        )\n",
    "        \"\"\"\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(128*6*6,2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 6)\n",
    "        )\n",
    "        \n",
    "        if copyParameters:\n",
    "            self.copyParameters(models.alexnet(pretrained=True))\n",
    "        \n",
    "        \n",
    "    def copyParameters(self, alexnetModel):\n",
    "        for i, a in enumerate(alexnetModel.features):\n",
    "            if type(a) is torch.nn.modules.conv.Conv2d:\n",
    "                self.features[i].weight = a.weight\n",
    "                self.features[i].bias   = a.bias\n",
    "    \n",
    "    def cuda(self, device=None):\n",
    "        self.isCuda = True\n",
    "        return self._apply(lambda t: t.cuda(device))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batchSize = len(x[0])\n",
    "        if self.r_type==\"lstm\":\n",
    "            outputs = []\n",
    "            \n",
    "            if self.isCuda:\n",
    "                ht1 = Variable( torch.zeros(batchSize,256,6,6)).cuda()\n",
    "                ct1 = Variable( torch.zeros(batchSize,256,6,6)).cuda()\n",
    "                ht2 = Variable( torch.zeros(batchSize,128,6,6)).cuda()\n",
    "                ct2 = Variable( torch.zeros(batchSize,128,6,6)).cuda()\n",
    "            else:\n",
    "                ht1 = Variable( torch.zeros(batchSize,256,6,6))\n",
    "                ct1 = Variable( torch.zeros(batchSize,256,6,6))\n",
    "                ht2 = Variable( torch.zeros(batchSize,128,6,6))\n",
    "                ct2 = Variable( torch.zeros(batchSize,128,6,6))\n",
    "                \n",
    "            for i in x:\n",
    "                xt = self.features(i)\n",
    "                o, (ht1,ct1) = self.convRNN1(xt, (ht1, ct1))\n",
    "                o, (ht2,ct2) = self.convRNN2(o, (ht2, ct2))\n",
    "                outputs.append(o)\n",
    "        \n",
    "        elif self.r_type==\"gru\":\n",
    "            outputs = []\n",
    "            if self.isCuda:\n",
    "                ht1 = Variable( torch.zeros(batchSize,256,6,6)).cuda()\n",
    "                ht2 = Variable( torch.zeros(batchSize,128,6,6)).cuda()\n",
    "            else:\n",
    "                ht1 = Variable( torch.zeros(batchSize,256,6,6))\n",
    "                ht2 = Variable( torch.zeros(batchSize,128,6,6))\n",
    "            \n",
    "            for e,i in enumerate(x):\n",
    "                xt = self.features(i)\n",
    "                ht1 = self.convRNN(xt, ht1)\n",
    "                ht2 = self.convRNN(ht1, ht2)\n",
    "                outputs.append(ht2)\n",
    "        return self.classifier(outputs[-1].view(batchSize, -1))\n",
    "        #x = self.classifier(x).squeeze().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    testconvLSTMCell()\n",
    "    testGruCell()\n",
    "    x = Variable(torch.Tensor(3,2,3,225,225)).cuda()\n",
    "    m = convRNN_1_layer(\"gru\").cuda()\n",
    "    print(m(x).size() == torch.Size([2,6]))\n",
    "    x = Variable(torch.Tensor(3,2,3,225,225)).cuda()\n",
    "    m = convRNN_2_layer(\"lstm\").cuda()\n",
    "    print(m(x).size() == torch.Size([2, 6]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "class ResNet_lstm(nn.Module):\n",
    "\n",
    "    def __init__(self, block=Bottleneck, layers=[3, 8, 36, 3]):\n",
    "        self.block = block\n",
    "        \n",
    "        self.inplanes = 64\n",
    "        super(ResNet_lstm, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        \n",
    "        self.isCuda = False\n",
    "        self.convRNN1 = ConvLSTMCell(512 * block.expansion, 512 * block.expansion,kernel_size=3, padding=1, stride=1)\n",
    "        self.convRNN2 = ConvLSTMCell(512 * block.expansion, 512 * block.expansion,kernel_size=3, padding=1, stride=1)\n",
    "        \n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        #self.fc = nn.Linear(512 * block.expansion, 6)\n",
    "        self.fc = nn.Linear(2048, 6)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def cuda(self, device=None):\n",
    "        self.isCuda = True\n",
    "        return self._apply(lambda t: t.cuda(device))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batchSize = x.size(1)\n",
    "        if self.isCuda:\n",
    "            ht1 = Variable( torch.zeros(batchSize, 512 * self.block.expansion,8,8)).cuda()\n",
    "            ct1 = Variable( torch.zeros(batchSize, 512 * self.block.expansion,8,8)).cuda()\n",
    "            ht2 = Variable( torch.zeros(batchSize, 512 * self.block.expansion,8,8)).cuda()\n",
    "            ct2 = Variable( torch.zeros(batchSize, 512 * self.block.expansion,8,8)).cuda()\n",
    "        else:\n",
    "            ht1 = Variable( torch.zeros(batchSize, 512 * self.block.expansion,8,8))\n",
    "            ct1 = Variable( torch.zeros(batchSize, 512 * self.block.expansion,8,8))\n",
    "            ht2 = Variable( torch.zeros(batchSize, 512 * self.block.expansion,8,8))\n",
    "            ct2 = Variable( torch.zeros(batchSize, 512 * self.block.expansion,8,8))\n",
    "        outputs = []\n",
    "        for i in x:\n",
    "            xt = self.conv1(i)\n",
    "            xt = self.bn1(xt)\n",
    "            xt = self.relu(xt)\n",
    "            xt = self.maxpool(xt)\n",
    "\n",
    "            xt = self.layer1(xt)\n",
    "            xt = self.layer2(xt)\n",
    "            xt = self.layer3(xt)\n",
    "            xt = self.layer4(xt)\n",
    "            \n",
    "            o, (ht1,ct1) = self.convRNN1(xt, (ht1, ct1))\n",
    "            o, (ht2,ct2) = self.convRNN2(o, (ht2, ct2))\n",
    "            outputs.append(o)\n",
    "\n",
    "        o = self.avgpool(outputs[-1])\n",
    "        o = o.view(batchSize, -1)\n",
    "        o = self.fc(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    x = Variable(torch.Tensor(3,2,3,225,225)).cuda()\n",
    "    m = ResNet_lstm(BasicBlock, [2, 2, 2, 2]).cuda()\n",
    "    print(m(x).size() == torch.Size([2, 6]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
