{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "#torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "#image\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "#jupyter\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "from __future__ import print_function\n",
    "\n",
    "#os\n",
    "import os\n",
    "import os.path as path\n",
    "import glob\n",
    "\n",
    "#math\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvLSTM\n",
    "\n",
    "#### LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=1, stride=1, bias=False):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        \n",
    "        self.k = kernel_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.w_i = nn.Parameter(torch.Tensor(4*out_channels, in_channels, kernel_size, kernel_size))\n",
    "        self.w_h = nn.Parameter(torch.Tensor(4*out_channels, out_channels, kernel_size, kernel_size))\n",
    "        self.w_c = nn.Parameter(torch.Tensor(3*out_channels, out_channels, kernel_size, kernel_size))\n",
    "\n",
    "        self.bias = bias\n",
    "        if bias:\n",
    "          self.bias_i = Parameter(torch.Tensor(4 * out_channels))\n",
    "          self.bias_h = Parameter(torch.Tensor(4 * out_channels))\n",
    "          self.bias_c = Parameter(torch.Tensor(3 * out_channels))\n",
    "        else:\n",
    "          self.register_parameter('bias_i', None)\n",
    "          self.register_parameter('bias_h', None)\n",
    "          self.register_parameter('bias_c', None)\n",
    "        \n",
    "        self.register_buffer('wc_blank', torch.zeros(out_channels))\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        n = 4 * self.in_channels * self.k * self.k\n",
    "        stdv = 1. / math.sqrt(n)\n",
    "        \n",
    "        self.w_i.data.uniform_(-stdv, stdv)\n",
    "        self.w_h.data.uniform_(-stdv, stdv)\n",
    "        self.w_c.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "        if self.bias:\n",
    "            self.bias_i.data.uniform_(-stdv, stdv)\n",
    "            self.bias_h.data.uniform_(-stdv, stdv)\n",
    "            self.bias_c.data.uniform_(-stdv, stdv)\n",
    "\n",
    "        \n",
    "    def forward(self, x, hx):\n",
    "        h, c = hx\n",
    "        print(self.w_h.size())\n",
    "        wx = F.conv2d(x, self.w_i, self.bias_i, padding=self.padding, stride=self.stride)\n",
    "        wh = F.conv2d(h, self.w_h, self.bias_h, padding=self.padding, stride=self.stride)\n",
    "        wc = F.conv2d(c, self.w_c, self.bias_c, padding=self.padding, stride=self.stride)\n",
    "        \n",
    "        \n",
    "        #wc = torch.cat((wc[:, :2 * self.out_channels], Variable(self.wc_blank).expand(wc.size(0), wc.size(1) // 3, wc.size(2), wc.size(3)), wc[:, 2 * self.out_channels:]), 1)\n",
    "        \n",
    "        i = F.sigmoid(wx[:, :self.out_channels] + wh[:, :self.out_channels] + wc[:, :self.out_channels])\n",
    "        f = F.sigmoid(wx[:, self.out_channels:2*self.out_channels] + wh[:, self.out_channels:2*self.out_channels] \n",
    "                + wc[:, self.out_channels:2*self.out_channels])\n",
    "        g = F.tanh(wx[:, 2*self.out_channels:3*self.out_channels] + wh[:, 2*self.out_channels:3*self.out_channels])\n",
    "        \"\"\"\n",
    "        \n",
    "        wxhc = wx + wh + torch.cat((wc[:, :2 * self.out_channels], Variable(self.wc_blank).expand(wc.size(0), wc.size(1) // 3, wc.size(2), wc.size(3)), wc[:, 2 * self.out_channels:]), 1)\n",
    "    \n",
    "        i = F.sigmoid(wxhc[:, :self.out_channels])\n",
    "        f = F.sigmoid(wxhc[:, self.out_channels:2 * self.out_channels])\n",
    "        g = F.tanh(wxhc[:, 2 * self.out_channels:3 * self.out_channels])\n",
    "        o = F.sigmoid(wxhc[:, 3 * self.out_channels:])\n",
    "        \"\"\"\n",
    "\n",
    "        c_t = f * c + i * g\n",
    "        o_t = F.sigmoid(wx[:, 3*self.out_channels:] + wh[:, 3*self.out_channels:] \n",
    "                        + wc[:, 2*self.out_channels: ]*c_t)\n",
    "        h_t = o_t * F.tanh(c_t)\n",
    "        \n",
    "        return h_t, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class Conv2dLSTMCell(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "    super(Conv2dLSTMCell, self).__init__()\n",
    "    if in_channels % groups != 0:\n",
    "        raise ValueError('in_channels must be divisible by groups')\n",
    "    if out_channels % groups != 0:\n",
    "        raise ValueError('out_channels must be divisible by groups')\n",
    "    kernel_size = _pair(kernel_size)\n",
    "    stride = _pair(stride)\n",
    "    padding = _pair(padding)\n",
    "    dilation = _pair(dilation)\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.kernel_size = kernel_size\n",
    "    self.stride = stride\n",
    "    self.padding = padding\n",
    "    self.padding_h = tuple(k // 2 for k, s, p, d in zip(kernel_size, stride, padding, dilation))\n",
    "    self.dilation = dilation\n",
    "    self.groups = groups\n",
    "    self.weight_ih = Parameter(torch.Tensor(4 * out_channels, in_channels // groups, *kernel_size))\n",
    "    self.weight_hh = Parameter(torch.Tensor(4 * out_channels, out_channels // groups, *kernel_size))\n",
    "    self.weight_ch = Parameter(torch.Tensor(3 * out_channels, out_channels // groups, *kernel_size))\n",
    "    if bias:\n",
    "      self.bias_ih = Parameter(torch.Tensor(4 * out_channels))\n",
    "      self.bias_hh = Parameter(torch.Tensor(4 * out_channels))\n",
    "      self.bias_ch = Parameter(torch.Tensor(3 * out_channels))\n",
    "    else:\n",
    "      self.register_parameter('bias_ih', None)\n",
    "      self.register_parameter('bias_hh', None)\n",
    "      self.register_parameter('bias_ch', None)\n",
    "    self.register_buffer('wc_blank', torch.zeros(out_channels))\n",
    "    self.reset_parameters()\n",
    "\n",
    "  def reset_parameters(self):\n",
    "    n = 4 * self.in_channels\n",
    "    for k in self.kernel_size:\n",
    "      n *= k\n",
    "    stdv = 1. / math.sqrt(n)\n",
    "    self.weight_ih.data.uniform_(-stdv, stdv)\n",
    "    self.weight_hh.data.uniform_(-stdv, stdv)\n",
    "    self.weight_ch.data.uniform_(-stdv, stdv)\n",
    "    if self.bias_ih is not None:\n",
    "      self.bias_ih.data.uniform_(-stdv, stdv)\n",
    "      self.bias_hh.data.uniform_(-stdv, stdv)\n",
    "      self.bias_ch.data.uniform_(-stdv, stdv)\n",
    "\n",
    "  def forward(self, i, hx):\n",
    "    h_0, c_0 = hx\n",
    "    print(self.weight_hh.size())\n",
    "    wx = F.conv2d(i, self.weight_ih, self.bias_ih, self.stride, self.padding, self.dilation, self.groups)\n",
    "    wh = F.conv2d(h_0, self.weight_hh, self.bias_hh, self.stride, self.padding_h, self.dilation, self.groups)\n",
    "    # Cell uses a Hadamard product instead of a convolution?\n",
    "    wc = F.conv2d(c_0, self.weight_ch, self.bias_ch, self.stride, self.padding_h, self.dilation, self.groups)\n",
    "    wxhc = wx + wh + torch.cat((wc[:, :2 * self.out_channels], Variable(self.wc_blank).expand(wc.size(0), wc.size(1) // 3, wc.size(2), wc.size(3)), wc[:, 2 * self.out_channels:]), 1)\n",
    "\n",
    "    i = F.sigmoid(wxhc[:, :self.out_channels])\n",
    "    f = F.sigmoid(wxhc[:, self.out_channels:2 * self.out_channels])\n",
    "    g = F.tanh(wxhc[:, 2 * self.out_channels:3 * self.out_channels])\n",
    "    o = F.sigmoid(wxhc[:, 3 * self.out_channels:])\n",
    "    \n",
    "    c_1 = f * c_0 + i * g\n",
    "    h_1 = o * F.tanh(c_1)\n",
    "    return h_1, (h_1, c_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ConvRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class convRNN_1_layer(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(convRNN_1_layer, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.convRNN = Conv2dLSTMCell(256,128,kernel_size=3, padding=1, stride=1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(256,6,kernel_size=1, padding=0, stride=1),\n",
    "            nn.AvgPool2d(kernel_size=6, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ht = Variable( torch.zeros(1,128,6,6))\n",
    "        ct = Variable( torch.zeros(1,128,6,6))\n",
    "        x = self.features(x)\n",
    "        return  self.convRNN(x, (ht, ct))\n",
    "        #x = self.classifier(x).squeeze().unsqueeze(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testconvRNN():\n",
    "    x = Variable(torch.Tensor(1,3,225,225))\n",
    "    m = convRNN_1_layer()\n",
    "    print(m(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 128, 3, 3])\n",
      "(Variable containing:\n",
      "( 0 , 0 ,.,.) = \n",
      "1.00000e-03 *\n",
      " -0.6927 -0.4737 -0.4859 -0.4975 -0.4914 -0.3841\n",
      " -0.9575  0.0636  0.0166  0.0037 -0.0145  0.0594\n",
      " -0.9212  0.0519  0.0131  0.0107 -0.0113  0.0406\n",
      " -0.9261  0.0618  0.0198  0.0136 -0.0148  0.0365\n",
      " -0.8768  0.1190  0.0726  0.0626  0.0220  0.0245\n",
      " -0.4727  0.4313  0.3779  0.3772  0.3600  0.0772\n",
      "\n",
      "( 0 , 1 ,.,.) = \n",
      "1.00000e-03 *\n",
      " -0.0542 -0.5659 -0.5734 -0.5710 -0.6102 -0.6753\n",
      "  0.3032 -0.3662 -0.3639 -0.3625 -0.3466 -0.6217\n",
      "  0.2723 -0.4251 -0.4000 -0.4061 -0.4005 -0.6372\n",
      "  0.2746 -0.4231 -0.4062 -0.4108 -0.4062 -0.6411\n",
      "  0.2832 -0.3731 -0.3832 -0.3835 -0.3736 -0.6256\n",
      "  0.1762 -0.5745 -0.5285 -0.5341 -0.5271 -0.5160\n",
      "\n",
      "( 0 , 2 ,.,.) = \n",
      "1.00000e-03 *\n",
      " -0.4142 -0.6181 -0.6394 -0.6357 -0.6244 -0.3590\n",
      "  0.6250  0.0437  0.0638  0.0616  0.0757 -0.2885\n",
      "  0.5277 -0.0850 -0.0755 -0.0735 -0.0844 -0.3836\n",
      "  0.5220 -0.0872 -0.0860 -0.0885 -0.0954 -0.4019\n",
      "  0.4702 -0.1569 -0.1357 -0.1413 -0.1526 -0.4455\n",
      "  0.5719  0.2484  0.2583  0.2548  0.2381  0.0048\n",
      "    ... \n",
      "\n",
      "( 0 ,125,.,.) = \n",
      "1.00000e-03 *\n",
      " -0.4183 -0.6436 -0.6236 -0.6163 -0.5376 -0.1832\n",
      " -0.7308 -1.2116 -1.1620 -1.1592 -1.0642 -0.7269\n",
      " -0.6710 -1.1273 -1.0758 -1.0759 -0.9583 -0.6855\n",
      " -0.6643 -1.1150 -1.0634 -1.0710 -0.9488 -0.6764\n",
      " -0.5831 -0.9857 -0.9289 -0.9293 -0.8126 -0.6148\n",
      " -0.4293 -0.7960 -0.7422 -0.7381 -0.6300 -0.9180\n",
      "\n",
      "( 0 ,126,.,.) = \n",
      "1.00000e-03 *\n",
      " -0.0376 -0.1955 -0.1740 -0.1817 -0.1579  0.1260\n",
      "  0.1332  0.0677  0.0554  0.0419  0.0475  0.4394\n",
      "  0.0685  0.0272  0.0242  0.0179  0.0476  0.4370\n",
      "  0.0564  0.0078  0.0054  0.0037  0.0388  0.4239\n",
      "  0.1263  0.0756  0.0701  0.0660  0.0685  0.4162\n",
      " -0.0086 -0.1817 -0.2000 -0.1989 -0.1564  0.0651\n",
      "\n",
      "( 0 ,127,.,.) = \n",
      "1.00000e-03 *\n",
      "  0.0894 -0.0395 -0.0985 -0.1062 -0.1090 -0.3720\n",
      " -0.5795 -0.4018 -0.4715 -0.4809 -0.5318 -0.6262\n",
      " -0.5757 -0.4536 -0.5158 -0.5261 -0.5960 -0.6537\n",
      " -0.5730 -0.4466 -0.5163 -0.5238 -0.5899 -0.6507\n",
      " -0.6002 -0.4738 -0.5588 -0.5638 -0.6349 -0.6480\n",
      " -0.7845 -0.4658 -0.5134 -0.5155 -0.6026 -0.5585\n",
      "[torch.FloatTensor of size 1x128x6x6]\n",
      ", (Variable containing:\n",
      "( 0 , 0 ,.,.) = \n",
      "1.00000e-03 *\n",
      " -0.6927 -0.4737 -0.4859 -0.4975 -0.4914 -0.3841\n",
      " -0.9575  0.0636  0.0166  0.0037 -0.0145  0.0594\n",
      " -0.9212  0.0519  0.0131  0.0107 -0.0113  0.0406\n",
      " -0.9261  0.0618  0.0198  0.0136 -0.0148  0.0365\n",
      " -0.8768  0.1190  0.0726  0.0626  0.0220  0.0245\n",
      " -0.4727  0.4313  0.3779  0.3772  0.3600  0.0772\n",
      "\n",
      "( 0 , 1 ,.,.) = \n",
      "1.00000e-03 *\n",
      " -0.0542 -0.5659 -0.5734 -0.5710 -0.6102 -0.6753\n",
      "  0.3032 -0.3662 -0.3639 -0.3625 -0.3466 -0.6217\n",
      "  0.2723 -0.4251 -0.4000 -0.4061 -0.4005 -0.6372\n",
      "  0.2746 -0.4231 -0.4062 -0.4108 -0.4062 -0.6411\n",
      "  0.2832 -0.3731 -0.3832 -0.3835 -0.3736 -0.6256\n",
      "  0.1762 -0.5745 -0.5285 -0.5341 -0.5271 -0.5160\n",
      "\n",
      "( 0 , 2 ,.,.) = \n",
      "1.00000e-03 *\n",
      " -0.4142 -0.6181 -0.6394 -0.6357 -0.6244 -0.3590\n",
      "  0.6250  0.0437  0.0638  0.0616  0.0757 -0.2885\n",
      "  0.5277 -0.0850 -0.0755 -0.0735 -0.0844 -0.3836\n",
      "  0.5220 -0.0872 -0.0860 -0.0885 -0.0954 -0.4019\n",
      "  0.4702 -0.1569 -0.1357 -0.1413 -0.1526 -0.4455\n",
      "  0.5719  0.2484  0.2583  0.2548  0.2381  0.0048\n",
      "    ... \n",
      "\n",
      "( 0 ,125,.,.) = \n",
      "1.00000e-03 *\n",
      " -0.4183 -0.6436 -0.6236 -0.6163 -0.5376 -0.1832\n",
      " -0.7308 -1.2116 -1.1620 -1.1592 -1.0642 -0.7269\n",
      " -0.6710 -1.1273 -1.0758 -1.0759 -0.9583 -0.6855\n",
      " -0.6643 -1.1150 -1.0634 -1.0710 -0.9488 -0.6764\n",
      " -0.5831 -0.9857 -0.9289 -0.9293 -0.8126 -0.6148\n",
      " -0.4293 -0.7960 -0.7422 -0.7381 -0.6300 -0.9180\n",
      "\n",
      "( 0 ,126,.,.) = \n",
      "1.00000e-03 *\n",
      " -0.0376 -0.1955 -0.1740 -0.1817 -0.1579  0.1260\n",
      "  0.1332  0.0677  0.0554  0.0419  0.0475  0.4394\n",
      "  0.0685  0.0272  0.0242  0.0179  0.0476  0.4370\n",
      "  0.0564  0.0078  0.0054  0.0037  0.0388  0.4239\n",
      "  0.1263  0.0756  0.0701  0.0660  0.0685  0.4162\n",
      " -0.0086 -0.1817 -0.2000 -0.1989 -0.1564  0.0651\n",
      "\n",
      "( 0 ,127,.,.) = \n",
      "1.00000e-03 *\n",
      "  0.0894 -0.0395 -0.0985 -0.1062 -0.1090 -0.3720\n",
      " -0.5795 -0.4018 -0.4715 -0.4809 -0.5318 -0.6262\n",
      " -0.5757 -0.4536 -0.5158 -0.5261 -0.5960 -0.6537\n",
      " -0.5730 -0.4466 -0.5163 -0.5238 -0.5899 -0.6507\n",
      " -0.6002 -0.4738 -0.5588 -0.5638 -0.6349 -0.6480\n",
      " -0.7845 -0.4658 -0.5134 -0.5155 -0.6026 -0.5585\n",
      "[torch.FloatTensor of size 1x128x6x6]\n",
      ", Variable containing:\n",
      "( 0 , 0 ,.,.) = \n",
      "1.00000e-03 *\n",
      " -1.3866 -0.9482 -0.9727 -0.9959 -0.9837 -0.7691\n",
      " -1.9163  0.1272  0.0333  0.0074 -0.0289  0.1188\n",
      " -1.8433  0.1039  0.0262  0.0214 -0.0227  0.0812\n",
      " -1.8531  0.1236  0.0397  0.0272 -0.0297  0.0730\n",
      " -1.7543  0.2378  0.1452  0.1251  0.0440  0.0489\n",
      " -0.9458  0.8631  0.7564  0.7549  0.7205  0.1544\n",
      "\n",
      "( 0 , 1 ,.,.) = \n",
      "1.00000e-03 *\n",
      " -0.1083 -1.1318 -1.1467 -1.1420 -1.2202 -1.3517\n",
      "  0.6065 -0.7324 -0.7278 -0.7249 -0.6931 -1.2445\n",
      "  0.5449 -0.8504 -0.8000 -0.8123 -0.8010 -1.2756\n",
      "  0.5494 -0.8464 -0.8126 -0.8218 -0.8123 -1.2834\n",
      "  0.5668 -0.7465 -0.7666 -0.7673 -0.7474 -1.2524\n",
      "  0.3522 -1.1487 -1.0566 -1.0680 -1.0538 -1.0320\n",
      "\n",
      "( 0 , 2 ,.,.) = \n",
      "1.00000e-03 *\n",
      " -0.8283 -1.2376 -1.2801 -1.2727 -1.2503 -0.7188\n",
      "  1.2509  0.0876  0.1279  0.1233  0.1516 -0.5775\n",
      "  1.0560 -0.1702 -0.1513 -0.1473 -0.1691 -0.7677\n",
      "  1.0445 -0.1746 -0.1723 -0.1772 -0.1910 -0.8044\n",
      "  0.9409 -0.3143 -0.2718 -0.2830 -0.3056 -0.8915\n",
      "  1.1439  0.4971  0.5169  0.5099  0.4767  0.0095\n",
      "    ... \n",
      "\n",
      "( 0 ,125,.,.) = \n",
      "1.00000e-03 *\n",
      " -0.8364 -1.2879 -1.2479 -1.2334 -1.0757 -0.3666\n",
      " -1.4616 -2.4249 -2.3257 -2.3200 -2.1300 -1.4548\n",
      " -1.3418 -2.2560 -2.1532 -2.1533 -1.9179 -1.3720\n",
      " -1.3284 -2.2314 -2.1282 -2.1433 -1.8990 -1.3539\n",
      " -1.1658 -1.9725 -1.8590 -1.8598 -1.6265 -1.2305\n",
      " -0.8586 -1.5925 -1.4849 -1.4765 -1.2605 -1.8362\n",
      "\n",
      "( 0 ,126,.,.) = \n",
      "1.00000e-03 *\n",
      " -0.0753 -0.3908 -0.3478 -0.3634 -0.3158  0.2515\n",
      "  0.2668  0.1355  0.1109  0.0838  0.0950  0.8778\n",
      "  0.1372  0.0545  0.0484  0.0357  0.0952  0.8727\n",
      "  0.1129  0.0156  0.0108  0.0073  0.0776  0.8466\n",
      "  0.2528  0.1511  0.1401  0.1320  0.1370  0.8311\n",
      " -0.0173 -0.3637 -0.4004 -0.3982 -0.3132  0.1302\n",
      "\n",
      "( 0 ,127,.,.) = \n",
      "1.00000e-03 *\n",
      "  0.1789 -0.0791 -0.1970 -0.2125 -0.2181 -0.7441\n",
      " -1.1583 -0.8033 -0.9427 -0.9614 -1.0634 -1.2528\n",
      " -1.1508 -0.9069 -1.0312 -1.0518 -1.1916 -1.3078\n",
      " -1.1452 -0.8929 -1.0322 -1.0472 -1.1795 -1.3019\n",
      " -1.1995 -0.9472 -1.1170 -1.1272 -1.2692 -1.2963\n",
      " -1.5685 -0.9319 -1.0272 -1.0314 -1.2058 -1.1179\n",
      "[torch.FloatTensor of size 1x128x6x6]\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "testconvRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
