{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "#torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "#torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "#image\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "#jupyter\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "#os\n",
    "import os\n",
    "import os.path as path\n",
    "import glob\n",
    "\n",
    "#math\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ConvLSTM\n",
    "\n",
    "#### LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "t = Variable(torch.rand(1,256,6,6))\n",
    "ht = Variable( torch.zeros(1,128,6,6))\n",
    "ct = Variable( torch.zeros(1,128,6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=1, stride=1, bias=False):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        \n",
    "        self.k = kernel_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.w_i = nn.Parameter(torch.Tensor(4*out_channels, in_channels, kernel_size, kernel_size))\n",
    "        self.w_h = nn.Parameter(torch.Tensor(4*out_channels, out_channels, kernel_size, kernel_size))\n",
    "        self.w_c = nn.Parameter(torch.Tensor(3*out_channels, out_channels, kernel_size, kernel_size))\n",
    "\n",
    "        self.bias = bias\n",
    "        if bias:\n",
    "          self.bias_i = Parameter(torch.Tensor(4 * out_channels))\n",
    "          self.bias_h = Parameter(torch.Tensor(4 * out_channels))\n",
    "          self.bias_c = Parameter(torch.Tensor(3 * out_channels))\n",
    "        else:\n",
    "          self.register_parameter('bias_i', None)\n",
    "          self.register_parameter('bias_h', None)\n",
    "          self.register_parameter('bias_c', None)\n",
    "        \n",
    "        self.register_buffer('wc_blank', torch.zeros(out_channels))\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        n = 4 * self.in_channels * self.k * self.k\n",
    "        stdv = 1. / math.sqrt(n)\n",
    "        \n",
    "        self.w_i.data.uniform_(-stdv, stdv)\n",
    "        self.w_h.data.uniform_(-stdv, stdv)\n",
    "        self.w_c.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "        if self.bias:\n",
    "            self.bias_i.data.uniform_(-stdv, stdv)\n",
    "            self.bias_h.data.uniform_(-stdv, stdv)\n",
    "            self.bias_c.data.uniform_(-stdv, stdv)\n",
    "\n",
    "        \n",
    "    def forward(self, x, hx):\n",
    "        h, c = hx\n",
    "        wx = F.conv2d(x, self.w_i, self.bias_i, padding=self.padding, stride=self.stride)\n",
    "        wh = F.conv2d(h, self.w_h, self.bias_h, padding=self.padding, stride=self.stride)\n",
    "        wc = F.conv2d(c, self.w_c, self.bias_c, padding=self.padding, stride=self.stride)\n",
    "        \n",
    "        \n",
    "        #wc = torch.cat((wc[:, :2 * self.out_channels], Variable(self.wc_blank).expand(wc.size(0), wc.size(1) // 3, wc.size(2), wc.size(3)), wc[:, 2 * self.out_channels:]), 1)\n",
    "        \n",
    "        i = F.sigmoid(wx[:, :self.out_channels] + wh[:, :self.out_channels] + wc[:, :self.out_channels])\n",
    "        f = F.sigmoid(wx[:, self.out_channels:2*self.out_channels] + wh[:, self.out_channels:2*self.out_channels] \n",
    "                + wc[:, self.out_channels:2*self.out_channels])\n",
    "        g = F.tanh(wx[:, 2*self.out_channels:3*self.out_channels] + wh[:, 2*self.out_channels:3*self.out_channels])\n",
    "        \"\"\"\n",
    "        \n",
    "        wxhc = wx + wh + torch.cat((wc[:, :2 * self.out_channels], Variable(self.wc_blank).expand(wc.size(0), wc.size(1) // 3, wc.size(2), wc.size(3)), wc[:, 2 * self.out_channels:]), 1)\n",
    "    \n",
    "        i = F.sigmoid(wxhc[:, :self.out_channels])\n",
    "        f = F.sigmoid(wxhc[:, self.out_channels:2 * self.out_channels])\n",
    "        g = F.tanh(wxhc[:, 2 * self.out_channels:3 * self.out_channels])\n",
    "        o = F.sigmoid(wxhc[:, 3 * self.out_channels:])\n",
    "        \"\"\"\n",
    "\n",
    "        c_t = f * c + i * g\n",
    "        o_t = F.sigmoid(wx[:, 3*self.out_channels:] + wh[:, 3*self.out_channels:] \n",
    "                        + wc[:, 2*self.out_channels: ]*c_t)\n",
    "        h_t = o_t * F.tanh(c_t)\n",
    "        \n",
    "        return h_t, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Test convLSTM Cell\n",
    "\"\"\"\n",
    "def testconvLSTMCell():\n",
    "    c = ConvLSTMCell(256,128,3)\n",
    "    o = c(t, (ht,ct))\n",
    "    print(o[0].size() == torch.Size([1,128,6,6]))\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import init\n",
    "\n",
    "class ConvGRUCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Generate a convolutional GRU cell\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, kernel_size):\n",
    "        super(ConvGRUCell, self).__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.reset_gate = nn.Conv2d(input_size + hidden_size, hidden_size, kernel_size, padding=padding)\n",
    "        self.update_gate = nn.Conv2d(input_size + hidden_size, hidden_size, kernel_size, padding=padding)\n",
    "        self.out_gate = nn.Conv2d(input_size + hidden_size, hidden_size, kernel_size, padding=padding)\n",
    "\n",
    "        init.orthogonal(self.reset_gate.weight)\n",
    "        init.orthogonal(self.update_gate.weight)\n",
    "        init.orthogonal(self.out_gate.weight)\n",
    "        init.constant(self.reset_gate.bias, 0.)\n",
    "        init.constant(self.update_gate.bias, 0.)\n",
    "        init.constant(self.out_gate.bias, 0.)\n",
    "\n",
    "\n",
    "    def forward(self, input_, prev_state):\n",
    "\n",
    "        # get batch and spatial sizes\n",
    "        batch_size = input_.data.size()[0]\n",
    "        spatial_size = input_.data.size()[2:]\n",
    "\n",
    "        # generate empty prev_state, if None is provided\n",
    "        if prev_state is None:\n",
    "            state_size = [batch_size, self.hidden_size] + list(spatial_size)\n",
    "            if torch.cuda.is_available():\n",
    "                prev_state = Variable(torch.zeros(state_size)).cuda()\n",
    "            else:\n",
    "                prev_state = Variable(torch.zeros(state_size))\n",
    "\n",
    "        # data size is [batch, channel, height, width]\n",
    "        stacked_inputs = torch.cat([input_, prev_state], dim=1)\n",
    "        update = F.sigmoid(self.update_gate(stacked_inputs))\n",
    "        reset = F.sigmoid(self.reset_gate(stacked_inputs))\n",
    "        out_inputs = F.tanh(self.out_gate(torch.cat([input_, prev_state * reset], dim=1)))\n",
    "        new_state = prev_state * (1 - update) + out_inputs * update\n",
    "\n",
    "        return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Test GruCell\n",
    "\"\"\"\n",
    "def testGruCell():\n",
    "    c = ConvGRUCell(256,128,3)\n",
    "    o = c(t, Variable( torch.zeros(1,128,6,6)))\n",
    "    print(o.size() == torch.Size([1,128,6,6]))\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### ConvRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class convRNN_1_layer(nn.Module):\n",
    "    \"\"\"\n",
    "        Define a RNN with 1 recurrent layer\n",
    "        args : r_type : lstm | gru\n",
    "    \"\"\"\n",
    "    def __init__(self, r_type=\"lstm\"):\n",
    "        super(convRNN_1_layer, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.r_type = r_type\n",
    "        if r_type == \"lstm\":\n",
    "            self.convRNN = ConvLSTMCell(256,128,kernel_size=3, padding=1, stride=1)\n",
    "        elif r_type == \"gru\":\n",
    "            self.convRNN = ConvGRUCell(256,128,3)\n",
    "        else:\n",
    "            print(\"Error : r_type\")\n",
    "            return -1\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(128,6,kernel_size=1, padding=0, stride=1),\n",
    "            nn.AvgPool2d(kernel_size=6, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        if self.r_type==\"lstm\":\n",
    "            outputs = []\n",
    "            ht = Variable( torch.zeros(1,128,6,6))\n",
    "            ct = Variable( torch.zeros(1,128,6,6))\n",
    "\n",
    "            for i in x:\n",
    "                xt = self.features(i)\n",
    "                o, (ht,ct) = self.convRNN(xt, (ht, ct))\n",
    "                outputs.append(o)\n",
    "        \n",
    "            return outputs[-1]\n",
    "        elif self.r_type==\"gru\":\n",
    "            outputs = []\n",
    "            ht = Variable( torch.zeros(1,128,6,6))\n",
    "            \n",
    "            for e,i in enumerate(x):\n",
    "                xt = self.features(i)\n",
    "                ht = self.convRNN(xt, ht)\n",
    "                outputs.append(ht)\n",
    "        \n",
    "            return outputs[-1]\n",
    "        #x = self.classifier(x).squeeze().unsqueeze(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def testconvRNN():\n",
    "    x = Variable(torch.Tensor(3,1,3,225,225))\n",
    "    m = convRNN_1_layer(\"gru\")\n",
    "    print(m(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
